# Codex configuration for Piano

################################################################################
# Core Model Selection
################################################################################

# Primary model used by Codex
model = "gpt-oss-120b"

# Provider id selected from [model_providers]
model_provider = "llama_swap"

# Default profile (overridden by CLI --profile)
profile = "smart"

# Reasoning effort: minimal | low | medium | high | xhigh
model_reasoning_effort = "high"

################################################################################
# Approval & Sandbox
################################################################################

# Filesystem/network sandbox policy for tool calls
sandbox_mode = "workspace-write"

################################################################################
# Sandbox settings
################################################################################

[sandbox_workspace_write]
# Allow outbound network access inside the sandbox
network_access = true

################################################################################
# Projects (trust levels)
################################################################################

[projects."/app"]
trust_level = "trusted"

################################################################################
# Model Providers
################################################################################

[model_providers]

# Local LLM provider (llama-swap, Ollama-compatible API)
[model_providers.llama_swap]
name = "Llama-swap"
base_url = "http://llama-swap:8080/v1"
wire_api = "chat"

# OpenAI provider for hosted models
[model_providers.openai]
name = "OpenAI"
base_url = "https://api.openai.com/v1"
env_key = "OPENAI_API_KEY"
wire_api = "chat"

################################################################################
# Profiles (named presets)
################################################################################

[profiles]

[profiles.smart]
model = "gpt-oss-120b"
model_provider = "llama_swap"
model_reasoning_effort = "high"

[profiles.fast]
model = "gpt-oss-20b"
model_provider = "llama_swap"
model_reasoning_effort = "medium"

[profiles.expensive]
model = "gpt-5.2-codex"
model_provider = "openai"
model_reasoning_effort = "high"

[profiles.replay]
model = "gpt-4"
model_provider = "openai"
model_reasoning_effort = "low"

[profiles.replay.model_providers.openai]
name = "OpenAI (Replay)"
# When running Codex in Docker, use host networking:
# - macOS/Windows: host.docker.internal works out of the box
# - Linux: add `--add-host=host.docker.internal:host-gateway`
base_url = "http://host.docker.internal:4000/v1"
wire_api = "responses"
