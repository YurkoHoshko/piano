# Codex configuration for Piano

################################################################################
# Core Model Selection
################################################################################

# Default profile (overridden by CLI --profile)
# NOTE: Do not set top-level model/model_provider here - they override profiles!

################################################################################
# Approval & Sandbox
################################################################################

# Filesystem/network sandbox policy for tool calls
sandbox_mode = "workspace-write"

################################################################################
# Sandbox settings
################################################################################

[sandbox_workspace_write]
# Allow outbound network access inside the sandbox
network_access = true

################################################################################
# Projects (trust levels)
################################################################################

[projects."/piano"]
trust_level = "trusted"

[projects."/piano/agents"]
trust_level = "trusted"

################################################################################
# Model Providers
################################################################################

[model_providers]

# Local LLM provider (llama-swap, Ollama-compatible API)
[model_providers.llama_swap]
name = "Llama-swap"
base_url = "http://llama-swap:8080/v1"
wire_api = "chat"

# OpenAI provider for hosted models
[model_providers.openai]
name = "OpenAI"
base_url = "https://api.openai.com/v1"

################################################################################
# Profiles (named presets)
################################################################################

[profiles]

[profiles.smart]
model = "gpt-oss-120b"
model_provider = "llama_swap"
model_reasoning_effort = "high"

[profiles.fast]
model = "gpt-oss-20b"
model_provider = "llama_swap"
model_reasoning_effort = "medium"

[profiles.expensive]
model = "gpt-5.2-codex"
model_provider = "openai"
model_reasoning_effort = "high"

[profiles.experimental]
model = "glm-47-flash"
model_provider = "llama_swap"
model_reasoning_effort = "high"

[profiles.gfq]
model = "glm-47-flash-q"
model_provider = "llama_swap"
model_reasoning_effort = "high"

[profiles.replay]
model = "gpt-4"
model_provider = "openai"
model_reasoning_effort = "low"

[profiles.replay.model_providers.openai]
name = "OpenAI (Replay)"
# When running Codex in Docker, use host networking:
# - macOS/Windows: host.docker.internal works out of the box
# - Linux: add `--add-host=host.docker.internal:host-gateway`
base_url = "http://host.docker.internal:4000/v1"
wire_api = "responses"

################################################################################
# Features Configuration
################################################################################

[features]
# Disable built-in view_image tool to force use of MCP vision tools
# The correct key is view_image_tool (not view_image)
view_image_tool = false

[mcp_servers.piano]
url = "http://piano:4000/mcp"
