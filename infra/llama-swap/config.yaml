logLevel: warn
startPort: 10001

macros:
  network_flags: >
    --host 0.0.0.0 --port ${PORT} -rtr --jinja -ngl 99
  context_flags: >
    -ub 1024 -c 32768
  gpt_oss_flags: >
    --temp 1 --top-p 1.0 --top-k 0
  qwen_coder_flags: >
    --temp 0.7 --top-p 0.8 --top-k 20
  devstral_flags: >
    --temp 0.15 --min-p 0.01
  glm_flags: >
    --temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1
  glm_q_flags: >
    --temp 0.7 --top-p 1.0 --min-p 0.01 --dry-multiplier 1.1

models:
  glm-47-flash:
    name: glm-47-flash
    cmd: |
      llama-server ${network_flags} ${context_flags} ${gpt_oss_flags} --n-cpu-moe 35 --model /models/glm-47-flash
    env:
      - CUDA_VISIBLE_DEVICES=0
    proxy: http://127.0.0.1:${PORT}
    checkEndpoint: /health

  glm-47-flash-q:
    name: glm-47-flash-q
    cmd: |
      llama-server ${network_flags} ${context_flags} ${glm_q_flags} --model /models/glm-47-flash-q
    env:
      - CUDA_VISIBLE_DEVICES=0,1
    proxy: http://127.0.0.1:${PORT}
    checkEndpoint: /health

  gpt-oss-20b:
    name: gpt-oss-20b
    cmd: |
      llama-server ${network_flags} ${context_flags} ${gpt_oss_flags} --model /models/gpt-oss-20b
    env:
      - CUDA_VISIBLE_DEVICES=0,1
    aliases:
      - gpt-oss
    proxy: http://127.0.0.1:${PORT}
    checkEndpoint: /health

  gpt-oss-120b:
    name: gpt-oss-120b
    cmd: |
      llama-server ${network_flags} ${context_flags} ${gpt_oss_flags} --cuda-params fusion=0 --model /models/gpt-oss-120b --cpu-moe
    proxy: http://127.0.0.1:${PORT}
    checkEndpoint: /health
    env:
      - CUDA_VISIBLE_DEVICES=0,1

  qwen3-coder.gguf:
    name: qwen3-coder.gguf
    cmd: |
      llama-server ${network_flags}  --cpu-moe ${context_flags} ${qwen_coder_flags} --jinja --model /models/qwen3-coder.gguf
    proxy: http://127.0.0.1:${PORT}
    checkEndpoint: /health
    env:
      - CUDA_VISIBLE_DEVICES=0,1

  nous-coder:
    name: nous-coder
    cmd: |
      llama-server ${network_flags} ${context_flags} ${qwen_coder_flags} --jinja --model /models/nous-coder
    proxy: http://127.0.0.1:${PORT}
    checkEndpoint: /health
    env:
      - CUDA_VISIBLE_DEVICES=0,1
      
  devstral-small-2-24b-instruct:
    name: devstral-small-2-24b-instruct
    cmd: |
      llama-server ${network_flags} ${context_flags} ${devstral_flags} --jinja --model /models/Devstral-Small-2-24B-Instruct-2512-IQ4_KSS.gguf
    proxy: http://127.0.0.1:${PORT}
    checkEndpoint: /health
    env:
      - CUDA_VISIBLE_DEVICES=0,1

hooks:
  on_startup:
    preload:
      - glm-47-flash
