logLevel: warn
startPort: 10001

macros:
  network_flags: >
    --host 0.0.0.0 --port ${PORT} -rtr --jinja -ngl 99
  context_flags: >
    -ub 1024 -c 32768
  gpt_oss_flags: >
    --temp 1 --top-p 1.0 --top-k 0
  qwen_coder_flags: >
    --temp 0.7 --top-p 0.8 --top-k 20
  devstral_flags: >
    --temp 0.15 --min-p 0.01
  glm_flags: >
    --temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1

models:
  glm-47-flash:
    name: glm-47-flash
    cmd: |
      llama-server ${network_flags} ${context_flags} ${gpt_oss_flags} --n-cpu-moe 35 --model /models/glm-47-flash
    env:
      - CUDA_VISIBLE_DEVICES=0
    proxy: http://127.0.0.1:${PORT}
    checkEndpoint: /health

  gpt-oss-20b:
    name: gpt-oss-20b
    cmd: |
      llama-server ${network_flags} ${context_flags} ${gpt_oss_flags} --model /models/gpt-oss-20b
    env:
      - CUDA_VISIBLE_DEVICES=0
    aliases:
      - gpt-oss
    proxy: http://127.0.0.1:${PORT}
    checkEndpoint: /health

  gpt-oss-120b:
    name: gpt-oss-120b
    cmd: |
      llama-server ${network_flags} ${context_flags} ${gpt_oss_flags} --cuda-params fusion=0 --model /models/gpt-oss-120b --cpu-moe
    proxy: http://127.0.0.1:${PORT}
    checkEndpoint: /health
    env:
      - CUDA_VISIBLE_DEVICES=0,1
hooks:
  on_startup:
    preload:
      - gpt-oss-20b
