# Voice & Vision Integration Architecture

## Overview
llama-swap now manages both traditional GGUF models (via llama.cpp) AND multimodal models (via vLLM) on the same port. Models swap automatically when requested.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    llama-swap (port 8000)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  llama.cpp      â”‚  â”‚     vLLM        â”‚  â”‚   vLLM       â”‚ â”‚
â”‚  â”‚  (GGUF models)  â”‚  â”‚   (ASR)         â”‚  â”‚  (Vision)    â”‚ â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚              â”‚ â”‚
â”‚  â”‚ â€¢ glm-47-flash  â”‚  â”‚ qwen3-asr-0.6b  â”‚  â”‚ qwen3-vl-4b  â”‚ â”‚
â”‚  â”‚ â€¢ devstral      â”‚  â”‚                 â”‚  â”‚              â”‚ â”‚
â”‚  â”‚ â€¢ qwen-coder    â”‚  â”‚ Voiceâ†’Text      â”‚  â”‚ Imageâ†’Text   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â”‚  All models share port 8000, swap on-demand                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Piano App (port 4000)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Telegram Bot    â”‚  â”‚   Codex Agent   â”‚                   â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚                   â”‚
â”‚  â”‚ Voice messages  â”‚  â”‚ LLM reasoning   â”‚                   â”‚
â”‚  â”‚ â†’ ASR (vLLM)    â”‚  â”‚                 â”‚                   â”‚
â”‚  â”‚                 â”‚  â”‚ Vision queries  â”‚                   â”‚
â”‚  â”‚ Image messages  â”‚  â”‚ â†’ VL (vLLM)     â”‚                   â”‚
â”‚  â”‚ â†’ VL (vLLM)     â”‚  â”‚                 â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Model Configuration

### ASR (Voice â†’ Text)
- **Model**: Qwen3-ASR-0.6B
- **Type**: vLLM (HuggingFace format)
- **GPU Memory**: 30% of GPU 0
- **Max Context**: 8192 tokens
- **Alias**: `asr`, `transcription`
- **Status**: âœ… Working

### Vision (Image â†’ Text)
- **Model**: Qwen3-VL-4B-Instruct
- **Type**: vLLM (HuggingFace format)
- **GPU Memory**: 40% of GPU 0
- **Max Context**: 16384 tokens
- **Images**: Up to 4 per prompt
- **Alias**: `vision`, `vl`, `ocr`
- **Status**: ğŸ”„ Model downloading

### Text Models (GGUF via llama.cpp)
- glm-47-flash
- glm-47-flash-q
- devstral
- qwen-coder
- etc.

## File Structure

```
~/.cache/llama.cpp/
â”œâ”€â”€ qwen3-asr/              # ASR model (1.8GB) âœ… Ready
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ model.safetensors
â”‚   â””â”€â”€ vocab.json
â”œâ”€â”€ qwen3-vl-vllm/          # Vision model (downloading)
â”‚   â”œâ”€â”€ config.json         âœ…
â”‚   â”œâ”€â”€ model-0000x-of-00004.safetensors  ğŸ”„
â”‚   â””â”€â”€ ...
â”œâ”€â”€ *.gguf                  # Text models (existing)
â””â”€â”€ mmproj-*.gguf           # Vision projections
```

## Usage Examples

### ASR (Voice Transcription)
```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-asr-0.6b",
    "messages": [{
      "role": "user",
      "content": [{
        "type": "audio_url",
        "audio_url": {"url": "https://example.com/audio.wav"}
      }]
    }]
  }'
```

### Vision (OCR/Image Understanding)
```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-vl-4b-vllm",
    "messages": [{
      "role": "user",
      "content": [
        {"type": "text", "text": "What's in this image?"},
        {"type": "image_url", "image_url": {"url": "https://example.com/image.png"}}
      ]
    }]
  }'
```

### Text Generation (existing)
```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "glm-47-flash",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

## Implementation Notes

1. **llama-swap manages swapping**: When you request a different model, llama-swap stops the current one and starts the new one
2. **vLLM and llama.cpp coexist**: Both run inside the same llama-swap container
3. **Port sharing**: All models share port 8000, managed by llama-swap proxy
4. **GPU allocation**: Both vLLM models use GPU 0 (they don't run simultaneously)

## Next Steps

1. âœ… ASR working - voice messages can be transcribed
2. ğŸ”„ Vision model downloading (will enable OCR and image understanding)
3. â³ TTS (Text-to-Speech) - for voice responses
4. â³ Integration with Telegram bot

## Files Modified

- `infra/llama-swap/config.yaml` - Added ASR and Vision vLLM models
- `docker-compose.yml` - Single llama-swap service (removed separate containers)
- `lib/piano/tools/transcription_client.ex` - ASR HTTP client
- `lib/piano/telegram/bot_v2.ex` - Voice message handling
